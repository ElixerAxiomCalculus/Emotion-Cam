# üé• EmotionCam

**EmotionCam** is a real-time web-based application that seamlessly integrates facial emotion recognition, tone analysis, and sentiment detection using live **camera** and **microphone** input. It's designed as a complete multimodal AI experience powered by state-of-the-art models for expressive human-computer interaction.

---

## üöÄ Features

* üé≠ **Facial Emotion Recognition**
  Detects emotions like *happy, sad, angry, surprised,* and more using the webcam with DeepFace.

* üéß **Voice-Based Sentiment & Tone Analysis**
  Converts speech to text using Vosk and performs tone classification via transformer models.

* üìä **Multimodal Emotion Analysis Dashboard**
  Displays real-time feedback with emotion bars, tone tags, confidence scores, and dynamic UI.

* ‚ö° **Lightweight & Fast**
  Uses efficient models for smooth real-time performance even on low-end machines.

* üîê **Privacy-First**
  Runs entirely on the client and local backend ‚Äì no data is sent to external servers.

---

## üß† Tech Stack

### üîπ Backend

* Python (Flask + Flask-SocketIO)
* Vosk Speech Recognition
* DeepFace for facial emotion analysis
* Hugging Face Transformers (`distilroberta`, `twitter-roberta`, etc.)

### üîπ Frontend

* React.js + Vite for blazing fast UI
* WebRTC APIs for camera/mic access
* Socket.IO for real-time streaming
* Animated emotion/tone visualization

---

## üìÇ Project Structure

```
EmotionCam/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ tts.py               # Speech input processing & tone/emotion classification
‚îÇ   ‚îú‚îÄ‚îÄ tone.py              # NLP-based sentiment analysis
‚îÇ   ‚îú‚îÄ‚îÄ emo.py               # Facial emotion recognition (DeepFace)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt     # Backend dependencies
‚îÇ   ‚îî‚îÄ‚îÄ vosk-model/          # Vosk STT model directory
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx, App.css
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ package.json, vite.config.js
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üì¶ Setup Instructions

### üîß Backend (Python)

1. **Set up virtual environment**

   ```bash
   cd backend
   python -m venv env
   env\Scripts\activate   # or source env/bin/activate on macOS/Linux
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Run Flask server**

   ```bash
   python tts.py
   ```

### üíª Frontend (React)

1. **Install dependencies**

   ```bash
   cd frontend
   npm install
   ```

2. **Run frontend**

   ```bash
   npm run dev
   ```

> Make sure both backend and frontend are running concurrently.

---


## üß™ Models Used

* üó£Ô∏è `vosk-model-small-en-us-0.15` ‚Äì offline speech-to-text
* ü§ñ `j-hartmann/emotion-english-distilroberta-base` ‚Äì emotion classification
* üß† `cardiffnlp/twitter-roberta-base-sentiment` ‚Äì sentiment analysis
* üòä DeepFace ‚Äì real-time facial emotion recognition

---

## ‚ú® Future Ideas

* üßë‚Äçüíº Personal Emotion Report Generator
* üìà Emotion trends & daily mood tracking
* üåê Multi-language support
* üéÆ Integration with games or VR for expressive gameplay

---

## ü§ù Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you‚Äôd like to change or add.

---

## üìú License

[MIT License](LICENSE)

---

## üë§ Author

**Sayak Mondal**
üîó [GitHub](https://github.com/ElixerAxiomCalculus) | üåê [www.aisayak.in](http://www.aisayak.in)

> Built with ‚ù§Ô∏è for real-time emotional insight and AI-driven human interaction.
